# Verdict User Guide

**Verdict** is a strict, CI-first regression gate for RAG pipelines (and other LLM apps). It is designed to be deterministic, fast, and easy to integrate into Pull Request workflows.

## ðŸš€ Quickstart

1. **Install Verdict** (via `cargo` or pre-built binary):
   ```bash
   cargo install --git https://github.com/Rul1an/verdict.git verdict-cli
   ```

2. **Initialize Scaffolding**:
   Run `init` to generate a ready-to-use CI setup, including a sample config and traces for deterministic replay.
   ```bash
   verdict init --ci --gitignore
   ```
   This creates:
   - `ci-eval.yaml`: Evaluation configuration.
   - `traces/ci.jsonl`: Pre-recorded LLM interactions (Replay Mode).
   - `schemas/ci_answer.schema.json`: Example JSON Schema.
   - `.github/workflows/verdict.yml`: GitHub Actions workflow.

3. **Run CI Gate**:
   ```bash
   verdict ci --config ci-eval.yaml --trace-file traces/ci.jsonl --strict
   ```

---

## ðŸ’¡ Core Concepts

### Statuses
Verdict tests result in one of five statuses:
- **Pass**: Metric matched expectation.
- **Fail**: Metric failed (e.g. regex didn't match).
- **Error**: System error (e.g. LLM call failed, config error, trace missing).
- **Warn**: Test failed, but is marked as `warn` in Quarantine (Non-blocking by default).
- **Flaky**: Test passed sometimes and failed sometimes (Auto-rerun detected).

### Strict Mode (`--strict`)
By default, `Warn` and `Flaky` statuses are treated as passing (Exit Code 0).
Use `--strict` to treat them as failures (Exit Code 1), enforcing a clean green state.

### Replay Mode vs Reruns
- **Replay Mode**: When `--trace-file` is provided, Verdict uses recorded LLM responses. This allows for **deterministic** and **fast** CI runs without API costs.
- **Reruns**: In live mode, Verdict can retry failed tests (`--rerun-failures N`) to detect flakiness. In Replay Mode, reruns are forced to 0.

### Path Resolution
File paths in configuration (e.g., `schema_file`) are resolved **relative to the configuration file**. This ensures your config works consistently whether run from the project root or a subdirectory.

---

## ðŸ“ Configuration (`eval.yaml`)

```yaml
version: 1
suite: "my_rag_suite"
model: "gpt-4o" # or "trace" for replay
tests:
  - id: "rag_q1"
    input:
      prompt: "Explain RAG"
    expected:
      # Option A: Regex Match
      type: regex_match
      pattern: "retrieval.*generation"
      flags: ["i"]

  - id: "json_output"
    input:
      prompt: "Output JSON"
    expected:
      # Option B: JSON Schema
      type: json_schema
      schema_file: "schemas/answer.schema.json" # Relative to eval.yaml
```

### Metrics
- `must_contain`: List of substrings that must be present.
- `must_not_contain`: List of substrings that must be absent.
- `regex_match` / `regex_not_match`: Perl-style regex validation.
- `json_schema`: Validates structure against a JSON Schema (inline or file).
- `semantic_similarity_to`: Fuzzy match using vector embeddings (requires `openai` embedder or trace metadata).

### Semantic Similarity (Fuzzy Match)
Use this metric to check if the output "means" the same as a reference string, even if the wording differs.

```yaml
    expected:
      type: semantic_similarity_to
      text: "The user is allowed to reset their password."
      threshold: 0.85 # (Default: 0.80)
```

**Running Locally (Live)**:
Requires `OPENAI_API_KEY`.
```bash
export OPENAI_API_KEY=sk-...
verdict run --embedder openai --embedding-model text-embedding-3-small
```

**Running in CI (Offline/Replay)**:
If you are using `--trace-file`, Verdict will look for pre-computed embeddings in the matching trace entry `meta.verdict.embeddings`.
This makes semantic tests **deterministic and free** in CI. No API calls are made if embeddings are present in the trace.

**Trace Schema (v1)**:
```json
{
  "request_id": "...",
  "prompt": "...",
  "response": "...",
  "meta": {
    "verdict": {
      "embeddings": {
        "model": "text-embedding-3-small",
        "response": [0.1, 0.2, ...],
        "reference": [0.3, 0.4, ...],
        "source_response": "live",
        "source_reference": "live"
      }
    }
  }
}
```

---

## ðŸ›¡ï¸ Privacy & Security

### `--redact-prompts`
Use this flag to redact the `prompt` field in all generated artifacts (SARIF, JSON, JUnit). This prevents PII or sensitive data from leaking into CI logs.

```bash
verdict ci ... --redact-prompts
```

---

## ðŸ“Š CI/CD Integration

Verdict is designed for GitHub Actions (and other CI systems).

### Reports
- `junit.xml`: Test results for CI UI integration. `Warn`/`Flaky` tests appear as "Passed" with warning logs (unless `--strict`).
- `sarif.json`: Static Analysis results for GitHub Code Scanning (showing failures inline in PRs).
- `run.json`: Full detailed JSON report.

### Workflow Example
See `.github/workflows/verdict.yml` generated by `verdict init --ci`.

---

## ðŸ”§ Troubleshooting

- **"config error: failed to read schema_file"**: Verdict prints both the *resolved absolute path* and the *original relative path* to help you debug file location issues in CI.
- "trace miss": In Replay Mode, this means the prompt requested is not found in the provided --trace-file.

---

## ðŸ“ˆ Baseline Workflow (Regression Testing)

For critical applications, defining absolute thresholds (e.g. "score > 0.8") is hard. It is often better to ensure that new changes **do not degrade** performance compared to the `main` branch.

### 3-Step Workflow

**1. Export Baseline (on main)**
In your CI pipeline for the `main` branch, run the tests and export the results as a baseline artifact.

```bash
verdict ci --export-baseline baseline.json
# Upload baseline.json as a CI artifact
```

**2. Compare (on PRs)**
In your Pull Request pipeline, download the baseline artifact from `main` and run the tests against it.

```bash
# Download baseline.json
verdict ci --baseline baseline.json
```

**3. Configure Thresholds**
In your `eval.yaml`, configure `thresholding` to define acceptable regression.

```yaml
settings:
  thresholding:
    mode: relative
    max_drop: 0.01  # Allow 1% drop
    # min_floor: 0.7 # Optional safety net
```

If a test score drops by more than `0.01` compared to the baseline, the test will **Fail**.

### GitHub Actions Example

```yaml
jobs:
  verdict:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # Step 1: Download Baseline (only on PRs)
      - name: Download Baseline
        if: github.event_name == 'pull_request'
        uses: actions/download-artifact@v4
        continue-on-error: true # It's okay if baseline doesn't exist yet
        with:
          name: verdict-baseline
          path: .

      # Step 2: Run Verdict (with conditional flags)
      - name: Verdict CI
        uses: Rul1an/verdict/verdict-action@v0.1.0
        with:
          # On PRs, use baseline if it exists
          baseline: ${{ github.event_name == 'pull_request' && hashFiles('baseline.json') != '' && 'baseline.json' || '' }}
          # On Main, export new baseline
          export_baseline: ${{ github.ref == 'refs/heads/main' && 'baseline.json' || '' }}

      # Step 3: Upload New Baseline (only on Main)
      - name: Upload Baseline
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: verdict-baseline
          path: baseline.json
          retention-days: 90
```
