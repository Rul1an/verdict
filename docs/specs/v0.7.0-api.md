# Verdict SDK v0.7.0 API Specification

**Goal**: SDK-native Evaluation (`from verdict_sdk import Evaluator`).

## 1. Evaluator API (`verdict_sdk/evaluator.py`)

```python
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Union
from .baseline import BaselineRef
from .config import EvalConfig
from .judge.client import JudgeClient
from .result import CompareResult, EvalRun

ConfigLike = Union[None, str, Path, Dict[str, Any], EvalConfig]
TraceLike = Union[str, Path]

@dataclass(frozen=True)
class EvaluatorOptions:
    workdir: Union[str, Path] = ".eval"
    cache: bool = True
    strict: bool = False
    baseline_overwrite: bool = False

class Evaluator:
    def __init__(
        self,
        config: ConfigLike = None,
        *,
        workdir: Union[str, Path] = ".eval",
        judge: Optional[JudgeClient] = None,
        cache: bool = True,
        strict: bool = False,
        baseline_overwrite: bool = False,
    ) -> None:
        """
        Progressive disclosure:
          - Evaluator() -> loads ./eval.yaml
          - Evaluator("path/eval.yaml") -> loads file
        """
        ...

    def run(
        self,
        trace: TraceLike,
        *,
        test_ids: Optional[Sequence[str]] = None,
        tags: Optional[Sequence[str]] = None,
    ) -> EvalRun:
        """Evaluate trace against config."""
        ...

    def compare(
        self,
        trace: TraceLike,
        *,
        baseline: str = "main",
        create_if_missing: bool = False,
        fail_on_regression: bool = True,
        test_ids: Optional[Sequence[str]] = None,
        tags: Optional[Sequence[str]] = None,
    ) -> CompareResult:
        """Compute deltas against baseline."""
        ...
```

## 2. Result API (`verdict_sdk/result.py`)

```python
@dataclass(frozen=True)
class CompareResult:
    passed: bool
    exit_code: int  # 0 pass, 1 regression, 2 error
    summary: str
    baseline: str
    current_run_id: str
    regressions: List[Regression]
    artifacts: ResultArtifacts

    def raise_for_status(self) -> None:
        ...
```

## 3. Config Model (`verdict_sdk/config.py`)

Supports both SDK-native and CLI-compat formats.

```python
@dataclass(frozen=True)
class EvalConfig:
    version: int = 1
    judge: JudgeConfig
    tests: List[TestSpec]
    meta: Dict[str, Any]
```

## 4. Artifact Layout

```
.eval/
  runs/
    <run_id>/
      run.json
      results.jsonl
      diff.json
  baselines/
    <name>/
      run.json
```

## 5. JSON Schemas

**run.json**:
Standardized run report containing `tests`, `metrics`, and `passed` status.

**diff.json**:
Contains `regressions` list with `delta` and `severity`.
