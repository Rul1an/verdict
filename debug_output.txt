/Users/roelschuurkes/miniconda3/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform darwin -- Python 3.12.9, pytest-8.3.5, pluggy-1.5.0
rootdir: /Users/roelschuurkes/verdict/verdict-sdk/python
configfile: pyproject.toml
plugins: anyio-4.9.0, dash-3.0.0, langsmith-0.3.13, asyncio-1.0.0
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 3 items

verdict-sdk/python/tests/test_async.py .DEBUG: Event 0: episode_start - None
DEBUG: Event 1: step - None
DEBUG: Event 2: tool_call - None
DEBUG: Event 3: tool_call - None
DEBUG: Event 4: step - None
DEBUG: Event 5: episode_end - None
F.

=================================== FAILURES ===================================
_________________________ test_record_with_tools_async _________________________

mock_writer = <test_async.MockWriter object at 0x11928bcb0>
mock_client = <AsyncMock id='4717067488'>
mock_clock = FrozenClock(t_ms=1000, step_ms=0)

    @pytest.mark.asyncio
    async def test_record_with_tools_async(mock_writer, mock_client, mock_clock):
        # Round 1: Assistant calls tool
        msg1 = MagicMock()
        msg1.content = "Thinking..."
        msg1.tool_calls = [MagicMock(id="call_1", function=MagicMock(name="my_tool", arguments='{"x": 1}'))]
    
        # Round 2: Final Answer
        msg2 = MagicMock()
        msg2.content = "Done"
        msg2.tool_calls = None
    
        # Setup side_effect for consecutive calls
        resp1 = MagicMock(choices=[MagicMock(message=msg1)], model="gpt-4")
        resp2 = MagicMock(choices=[MagicMock(message=msg2)], model="gpt-4")
    
        mock_client.chat.completions.create.side_effect = [resp1, resp2]
    
        # Async Tool Executor
        async def my_tool(args):
            return {"y": args["x"] + 1}
    
        result = await record_chat_completions_with_tools(
            writer=mock_writer,
            client=mock_client,
            model="gpt-4",
            messages=[{"role": "user", "content": "do tool"}],
            tool_executors={"my_tool": my_tool},
            episode_id="async_tools",
            clock=mock_clock
        )
    
        assert result["content"] == "Done"
    
        # Events: Start(1) + Model(1) + ToolCall(1) + ToolResult(1) + Model(2) + End(1) = 6 events
        for idx, e in enumerate(mock_writer.events):
            print(f"DEBUG: Event {idx}: {e.get('type')} - {e.get('tool_call_id')}")
    
        assert len(mock_writer.events) == 6
    
        # Check Tool Result Event (previously index 3, verifying)
        res_evt = mock_writer.events[3]
>       assert res_evt["type"] == "tool_call_result"
E       AssertionError: assert 'tool_call' == 'tool_call_result'
E         
E         - tool_call_result
E         + tool_call

verdict-sdk/python/tests/test_async.py:114: AssertionError
=========================== short test summary info ============================
FAILED verdict-sdk/python/tests/test_async.py::test_record_with_tools_async
========================= 1 failed, 2 passed in 0.05s ==========================
