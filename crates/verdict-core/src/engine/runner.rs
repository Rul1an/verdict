use crate::attempts::{classify_attempts, FailureClass};
use crate::cache::key::cache_key;
use crate::cache::vcr::VcrCache;
use crate::metrics_api::Metric;
use crate::model::{AttemptRow, EvalConfig, LlmResponse, TestCase, TestResultRow, TestStatus};
use crate::providers::llm::LlmClient;
use crate::quarantine::{QuarantineMode, QuarantineService};
use crate::report::RunArtifacts;
use crate::storage::Store;
use std::sync::Arc;
use tokio::sync::Semaphore;
use tokio::time::{timeout, Duration};

#[derive(Debug, Clone)]
pub struct RunPolicy {
    pub rerun_failures: u32,
    pub quarantine_mode: QuarantineMode,
}

impl Default for RunPolicy {
    fn default() -> Self {
        Self {
            rerun_failures: 1,
            quarantine_mode: QuarantineMode::Warn,
        }
    }
}

pub struct Runner {
    pub store: Store,
    pub cache: VcrCache,
    pub client: Arc<dyn LlmClient>,
    pub metrics: Vec<Arc<dyn Metric>>,
    pub policy: RunPolicy,
    pub embedder: Option<Arc<dyn crate::providers::embedder::Embedder>>,
    pub refresh_embeddings: bool,
    pub judge: Option<crate::judge::JudgeService>,
    pub baseline: Option<crate::baseline::Baseline>,
}

impl Runner {
    pub async fn run_suite(&self, cfg: &EvalConfig) -> anyhow::Result<RunArtifacts> {
        let run_id = self.store.create_run(cfg)?;

        let parallel = cfg.settings.parallel.unwrap_or(4).max(1);
        let sem = Arc::new(Semaphore::new(parallel));
        let mut handles = Vec::new();

        for tc in cfg.tests.iter() {
            let permit = sem.clone().acquire_owned().await?;
            let this = self.clone_for_task();
            let cfg = cfg.clone();
            let tc = tc.clone();
            let h = tokio::spawn(async move {
                let _permit = permit;
                this.run_test_with_policy(&cfg, &tc, run_id).await
            });
            handles.push(h);
        }

        let mut rows = Vec::new();
        let mut any_fail = false;
        for h in handles {
            let row = match h.await {
                Ok(Ok(row)) => row,
                Ok(Err(e)) => TestResultRow {
                    test_id: "unknown".into(),
                    status: TestStatus::Error,
                    score: None,
                    cached: false,
                    message: format!("task error: {}", e),
                    details: serde_json::json!({}),
                    duration_ms: None,
                },
                Err(e) => TestResultRow {
                    test_id: "unknown".into(),
                    status: TestStatus::Error,
                    score: None,
                    cached: false,
                    message: format!("join error: {}", e),
                    details: serde_json::json!({}),
                    duration_ms: None,
                },
            };
            any_fail = any_fail || matches!(row.status, TestStatus::Fail | TestStatus::Error);
            rows.push(row);
        }

        self.store
            .finalize_run(run_id, if any_fail { "failed" } else { "passed" })?;
        Ok(RunArtifacts {
            run_id,
            suite: cfg.suite.clone(),
            results: rows,
        })
    }

    async fn run_test_with_policy(
        &self,
        cfg: &EvalConfig,
        tc: &TestCase,
        run_id: i64,
    ) -> anyhow::Result<TestResultRow> {
        let quarantine = QuarantineService::new(self.store.clone());
        let q_reason = quarantine.is_quarantined(&cfg.suite, &tc.id)?;

        let max_attempts = 1 + self.policy.rerun_failures;
        let mut attempts: Vec<AttemptRow> = Vec::new();
        let mut last_row: Option<TestResultRow> = None;
        let mut last_output: Option<LlmResponse> = None;

        for i in 0..max_attempts {
            let (row, output) = self.run_test_once(cfg, tc).await?;
            attempts.push(AttemptRow {
                attempt_no: i + 1,
                status: row.status.clone(),
                message: row.message.clone(),
                duration_ms: row.duration_ms,
                details: row.details.clone(),
            });
            last_row = Some(row.clone());
            last_output = Some(output.clone());

            match row.status {
                TestStatus::Pass | TestStatus::Warn => break,
                TestStatus::Fail | TestStatus::Error | TestStatus::Flaky => continue,
            }
        }

        let class = classify_attempts(&attempts);
        let mut final_row = last_row.unwrap_or(TestResultRow {
            test_id: tc.id.clone(),
            status: TestStatus::Error,
            score: None,
            cached: false,
            message: "no attempts".into(),
            details: serde_json::json!({}),
            duration_ms: None,
        });

        // quarantine overlay
        if let Some(reason) = q_reason {
            match self.policy.quarantine_mode {
                QuarantineMode::Off => {}
                QuarantineMode::Warn => {
                    final_row.status = TestStatus::Warn;
                    final_row.message = format!("quarantined: {}", reason);
                }
                QuarantineMode::Strict => {
                    final_row.status = TestStatus::Fail;
                    final_row.message = format!("quarantined (strict): {}", reason);
                }
            }
        }

        match class {
            FailureClass::Flake => {
                final_row.status = TestStatus::Flaky;
                final_row.message = "flake detected (rerun passed)".into();
                final_row.details["flake"] = serde_json::json!({ "attempts": attempts.len() });
            }
            FailureClass::Error => final_row.status = TestStatus::Error,
            FailureClass::DeterministicFail => {}
        }

        let output = last_output.unwrap_or(LlmResponse {
            text: "".into(),
            provider: self.client.provider_name().to_string(),
            model: cfg.model.clone(),
            cached: false,
            meta: serde_json::json!({}),
        });
        self.store
            .insert_result_embedded(run_id, &final_row, &attempts, &output)?;

        Ok(final_row)
    }

    async fn run_test_once(
        &self,
        cfg: &EvalConfig,
        tc: &TestCase,
    ) -> anyhow::Result<(TestResultRow, LlmResponse)> {
        let fingerprint = format!("v{}|{}", cfg.version, self.client.provider_name());
        let key = cache_key(&cfg.model, &tc.input.prompt, &fingerprint);

        let start = std::time::Instant::now();
        let mut cached = false;

        let mut resp: LlmResponse = if cfg.settings.cache.unwrap_or(true) {
            if let Some(r) = self.cache.get(&key)? {
                cached = true;
                r
            } else {
                let r = self.call_llm(cfg, tc).await?;
                self.cache.put(&key, &r)?;
                r
            }
        } else {
            self.call_llm(cfg, tc).await?
        };
        resp.cached = resp.cached || cached;

        // Semantic Enrichment
        self.enrich_semantic(tc, &mut resp).await?;
        self.enrich_judge(tc, &mut resp).await?;

        let mut final_status = TestStatus::Pass;
        let mut final_score: Option<f64> = None;
        let mut msg = String::new();
        let mut details = serde_json::json!({ "metrics": {} });

        for m in &self.metrics {
            let r = m.evaluate(tc, &tc.expected, &resp).await?;
            details["metrics"][m.name()] = serde_json::json!({
                "score": r.score, "passed": r.passed, "unstable": r.unstable, "details": r.details
            });
            final_score = Some(r.score);

            if r.unstable {
                // gate stability first: treat unstable as warn in MVP
                final_status = TestStatus::Warn;
                msg = format!("unstable metric: {}", m.name());
                break;
            }
            if !r.passed {
                final_status = TestStatus::Fail;
                msg = format!("failed: {}", m.name());
                break;
            }
        }

        // Post-metric baseline check
        if let Some(baseline) = &self.baseline {
            if let Some((new_status, new_msg)) =
                self.check_baseline_regressions(tc, cfg, &details, &self.metrics, baseline)
            {
                if matches!(new_status, TestStatus::Fail | TestStatus::Warn) {
                    final_status = new_status;
                    msg = new_msg;
                }
            }
        }

        let duration_ms = start.elapsed().as_millis() as u64;
        let mut row = TestResultRow {
            test_id: tc.id.clone(),
            status: final_status,
            score: final_score,
            cached: resp.cached,
            message: if msg.is_empty() { "ok".into() } else { msg },
            details,
            duration_ms: Some(duration_ms),
        };

        if self.client.provider_name() == "trace" {
            row.details["verdict.replay"] = serde_json::json!(true);
        }
        Ok((row, resp))
    }

    async fn call_llm(&self, cfg: &EvalConfig, tc: &TestCase) -> anyhow::Result<LlmResponse> {
        let t = cfg.settings.timeout_seconds.unwrap_or(30);
        let fut = self
            .client
            .complete(&tc.input.prompt, tc.input.context.as_deref());
        let resp = timeout(Duration::from_secs(t), fut).await??;
        Ok(resp)
    }

    fn clone_for_task(&self) -> RunnerRef {
        RunnerRef {
            store: self.store.clone(),
            cache: self.cache.clone(),
            client: self.client.clone(),
            metrics: self.metrics.clone(),
            policy: self.policy.clone(),
            embedder: self.embedder.clone(),
            refresh_embeddings: self.refresh_embeddings,
            judge: self.judge.clone(),
            baseline: self.baseline.clone(),
        }
    }

    fn check_baseline_regressions(
        &self,
        tc: &TestCase,
        cfg: &EvalConfig,
        details: &serde_json::Value,
        metrics: &[Arc<dyn Metric>],
        baseline: &crate::baseline::Baseline,
    ) -> Option<(TestStatus, String)> {
        // Check suite-level defaults
        let suite_defaults = cfg.settings.thresholding.as_ref();

        for m in metrics {
            let metric_name = m.name();
            // Only numeric metrics supported right now
            let score = details["metrics"][metric_name]["score"].as_f64()?;

            // Determine thresholding config
            // 1. Metric override (from expected enum - tricky as Metric trait hides this)
            // For now, let's assume we use suite defaults unless specific metric logic (TODO: pass config down)
            // Actually, `tc.expected` has the config. We need to parse it.

            let (mode, max_drop) = self.resolve_threshold_config(tc, metric_name, suite_defaults);

            if mode == "relative" {
                if let Some(base_score) = baseline.get_score(&tc.id, metric_name) {
                    let delta = score - base_score;
                    if let Some(drop_limit) = max_drop {
                        if delta < -drop_limit {
                            return Some((
                                TestStatus::Fail,
                                format!(
                                    "regression: {} dropped {:.3} (limit: {:.3})",
                                    metric_name, -delta, drop_limit
                                ),
                            ));
                        }
                    }
                } else {
                    // Missing baseline
                    return Some((
                        TestStatus::Warn,
                        format!("missing baseline for {}/{}", tc.id, metric_name),
                    ));
                }
            }
        }
        None
    }

    fn resolve_threshold_config(
        &self,
        tc: &TestCase,
        metric_name: &str,
        suite_defaults: Option<&crate::model::ThresholdingSettings>,
    ) -> (String, Option<f64>) {
        // Defaults
        let mut mode = "absolute".to_string();
        let mut max_drop = None;

        if let Some(s) = suite_defaults {
            if let Some(m) = &s.mode {
                mode = m.clone();
            }
            max_drop = s.max_drop;
        }

        // Per-metric overrides
        // Provide a clumsy match here or better implement a helper on Expected
        // For MVP, we'll check suite defaults primarily.
        // Actual implementation requires mapping metric_name to Expected variant fields.
        // Let's stick to suite defaults for this iteration to get it compiling.
        (mode, max_drop)
    }

    // Embeddings logic
    async fn enrich_semantic(&self, tc: &TestCase, resp: &mut LlmResponse) -> anyhow::Result<()> {
        use crate::model::Expected;

        let Expected::SemanticSimilarityTo {
            semantic_similarity_to,
            ..
        } = &tc.expected
        else {
            return Ok(());
        };

        // If trace already provided both vectors, accept them
        if resp.meta.pointer("/verdict/embeddings/response").is_some()
            && resp.meta.pointer("/verdict/embeddings/reference").is_some()
        {
            return Ok(());
        }

        let embedder = self.embedder.as_ref().ok_or_else(|| {
            anyhow::anyhow!(
                "config error: semantic_similarity_to requires an embedder (--embedder) or trace meta embeddings"
            )
        })?;

        let model_id = embedder.model_id();

        let (resp_vec, src_resp) = self
            .embed_text(&model_id, embedder.as_ref(), &resp.text)
            .await?;
        let (ref_vec, src_ref) = self
            .embed_text(&model_id, embedder.as_ref(), semantic_similarity_to)
            .await?;

        // write into meta.verdict.embeddings
        if !resp.meta.get("verdict").is_some_and(|v| v.is_object()) {
            resp.meta["verdict"] = serde_json::json!({});
        }
        resp.meta["verdict"]["embeddings"] = serde_json::json!({
            "model": model_id,
            "response": resp_vec,
            "reference": ref_vec,
            "source_response": src_resp,
            "source_reference": src_ref
        });

        Ok(())
    }

    pub async fn embed_text(
        &self,
        model_id: &str,
        embedder: &dyn crate::providers::embedder::Embedder,
        text: &str,
    ) -> anyhow::Result<(Vec<f32>, &'static str)> {
        use crate::embeddings::util::embed_cache_key;

        let key = embed_cache_key(model_id, text);

        if !self.refresh_embeddings {
            if let Some((_m, vec)) = self.store.get_embedding(&key)? {
                return Ok((vec, "cache"));
            }
        }

        let vec = embedder.embed(text).await?;
        self.store.put_embedding(&key, model_id, &vec)?;
        Ok((vec, "live"))
    }

    async fn enrich_judge(&self, tc: &TestCase, resp: &mut LlmResponse) -> anyhow::Result<()> {
        use crate::model::Expected;

        let (rubric_id, rubric_version) = match &tc.expected {
            Expected::Faithfulness { rubric_version, .. } => {
                ("faithfulness", rubric_version.as_deref())
            }
            Expected::Relevance { rubric_version, .. } => ("relevance", rubric_version.as_deref()),
            _ => return Ok(()),
        };

        let judge = self.judge.as_ref().ok_or_else(|| {
            anyhow::anyhow!("config error: judge required but service not initialized")
        })?;

        judge
            .evaluate(
                &tc.id,
                rubric_id,
                &tc.input,
                &resp.text,
                rubric_version,
                &mut resp.meta,
            )
            .await?;

        Ok(())
    }
}

#[derive(Clone)]
struct RunnerRef {
    store: Store,
    cache: VcrCache,
    client: Arc<dyn LlmClient>,
    metrics: Vec<Arc<dyn Metric>>,
    policy: RunPolicy,
    embedder: Option<Arc<dyn crate::providers::embedder::Embedder>>,
    refresh_embeddings: bool,
    judge: Option<crate::judge::JudgeService>,
    baseline: Option<crate::baseline::Baseline>,
}

impl RunnerRef {
    async fn run_test_with_policy(
        &self,
        cfg: &EvalConfig,
        tc: &TestCase,
        run_id: i64,
    ) -> anyhow::Result<TestResultRow> {
        let runner = Runner {
            store: self.store.clone(),
            cache: self.cache.clone(),
            client: self.client.clone(),
            metrics: self.metrics.clone(),
            policy: self.policy.clone(),
            embedder: self.embedder.clone(),
            refresh_embeddings: self.refresh_embeddings,
            judge: self.judge.clone(),
            baseline: self.baseline.clone(),
        };
        runner.run_test_with_policy(cfg, tc, run_id).await
    }
}
